{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small image captioning model\n",
    "\n",
    "From http://datascience.stackexchange.com/questions/10368/image-captioning-in-keras\n",
    "\n",
    "## TODO:\n",
    "\n",
    "* imports\n",
    "* loading VGG16 + weights\n",
    "* find + load training data (Flickr?)\n",
    "* debug :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_caption_len = 16\n",
    "vocab_size = 10000\n",
    "\n",
    "# first, let's define an image model that\n",
    "# will encode pictures into 128-dimensional vectors.\n",
    "# it should be initialized with pre-trained weights.\n",
    "image_model = VGG-16 CNN definition\n",
    "image_model.load_weights('weight_file.h5')\n",
    "\n",
    "# next, let's define a RNN model that encodes sequences of words\n",
    "# into sequences of 128-dimensional word vectors.\n",
    "language_model = Sequential()\n",
    "language_model.add(Embedding(vocab_size, 256, input_length=max_caption_len))\n",
    "language_model.add(GRU(output_dim=128, return_sequences=True))\n",
    "language_model.add(TimeDistributedDense(128))\n",
    "\n",
    "# let's repeat the image vector to turn it into a sequence.\n",
    "image_model.add(RepeatVector(max_caption_len))\n",
    "\n",
    "# the output of both models will be tensors of shape (samples, max_caption_len, 128).\n",
    "# let's concatenate these 2 vector sequences.\n",
    "model = Merge([image_model, language_model], mode='concat', concat_axis=-1)\n",
    "# let's encode this vector sequence into a single vector\n",
    "model.add(GRU(256, 256, return_sequences=False))\n",
    "# which will be used to compute a probability\n",
    "# distribution over what the next word in the caption should be!\n",
    "model.add(Dense(vocab_size))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "\n",
    "# \"images\" is a numpy float array of shape (nb_samples, width, height, nb_channels=3).\n",
    "# \"captions\" is a numpy integer array of shape (nb_samples, max_caption_len)\n",
    "# containing word index sequences representing partial captions.\n",
    "# \"next_words\" is a numpy float array of shape (nb_samples, vocab_size)\n",
    "# containing a categorical encoding (0s and 1s) of the next word in the corresponding\n",
    "# partial caption.\n",
    "model.fit([images, partial_captions], next_words, batch_size=16, nb_epoch=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
